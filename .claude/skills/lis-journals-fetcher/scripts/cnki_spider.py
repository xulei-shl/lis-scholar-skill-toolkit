#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CNKI æœŸåˆŠå¯¼èˆªçˆ¬è™«
ä½¿ç”¨ Playwright å®ç°çš„ä¸­å›½çŸ¥ç½‘æœŸåˆŠè®ºæ–‡çˆ¬å–å·¥å…·

åŠŸèƒ½ï¼š
1. çˆ¬å–æŒ‡å®šæœŸåˆŠçš„æŸä¸€æœŸè®ºæ–‡åˆ—è¡¨
2. å¯é€‰æ‹©æ˜¯å¦è·å–è®ºæ–‡æ‘˜è¦ç­‰è¯¦ç»†ä¿¡æ¯
3. æ”¯æŒå‘½ä»¤è¡Œå‚æ•°é…ç½®
"""

import argparse
import asyncio
import json
import sys
import time
from pathlib import Path
from typing import Optional, List, Union, Callable
from urllib.parse import urlparse

from playwright.async_api import async_playwright, TimeoutError as AsyncPlaywrightTimeoutError
from playwright.sync_api import sync_playwright, TimeoutError as PlaywrightTimeoutError

from paper_detail import PaperDetailSpider, AsyncPaperDetailSpider, ProgressReporter
from json_sanitizer import JSONSanitizer


class CNKISpider:
    """CNKI æœŸåˆŠçˆ¬è™«ç±»"""

    @staticmethod
    def parse_issue_string(issue_str: str) -> List[int]:
        """
        è§£ææœŸæ•°å­—ç¬¦ä¸²ï¼Œæ”¯æŒå¤šç§æ ¼å¼

        æ”¯æŒçš„æ ¼å¼ï¼š
        - å•æœŸ: "3" -> [3]
        - èŒƒå›´: "1-3" -> [1, 2, 3]
        - ç¦»æ•£: "1,5,7" -> [1, 5, 7]
        - æ··åˆ: "1-3,5,7-9" -> [1, 2, 3, 5, 7, 8, 9]

        Args:
            issue_str: æœŸæ•°å­—ç¬¦ä¸²

        Returns:
            æœŸå·åˆ—è¡¨

        Raises:
            ValueError: æœŸå·æ ¼å¼æ— æ•ˆæˆ–è¶…å‡ºèŒƒå›´
        """
        issues = set()

        # å»é™¤ç©ºæ ¼
        issue_str = issue_str.strip()

        # æŒ‰é€—å·åˆ†å‰²
        parts = issue_str.split(',')

        for part in parts:
            part = part.strip()
            if not part:
                continue

            # æ£€æŸ¥æ˜¯å¦æ˜¯èŒƒå›´æ ¼å¼ (å¦‚ "1-3")
            if '-' in part:
                range_parts = part.split('-')
                if len(range_parts) != 2:
                    raise ValueError(f"æ— æ•ˆçš„èŒƒå›´æ ¼å¼: {part}")

                start = int(range_parts[0].strip())
                end = int(range_parts[1].strip())

                if start > end:
                    raise ValueError(f"èŒƒå›´èµ·å§‹å€¼ä¸èƒ½å¤§äºç»“æŸå€¼: {part}")

                for issue in range(start, end + 1):
                    if not 1 <= issue <= 12:
                        raise ValueError(f"æœŸå· {issue} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (1-12)")
                    issues.add(issue)
            else:
                # å•ä¸ªæœŸå·
                issue = int(part)
                if not 1 <= issue <= 12:
                    raise ValueError(f"æœŸå· {issue} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (1-12)")
                issues.add(issue)

        return sorted(list(issues))

    def __init__(self, url: str, year: int, issues: Union[int, str, List[int]], get_details: bool = False,
                 headless: bool = True, timeout: int = 30000):
        """
        åˆå§‹åŒ–çˆ¬è™«

        Args:
            url: æœŸåˆŠå¯¼èˆªé¡µ URL
            year: å¹´ä»½
            issues: æœŸå·ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼:
                - æ•´æ•°: 3 (å•æœŸ)
                - å­—ç¬¦ä¸²: "3", "1-3", "1,5,7", "1-3,5,7-9"
                - åˆ—è¡¨: [3], [1, 2, 3], [1, 5, 7]
            get_details: æ˜¯å¦è·å–è®ºæ–‡æ‘˜è¦è¯¦æƒ…
            headless: æ˜¯å¦æ— å¤´æ¨¡å¼è¿è¡Œ
            timeout: è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰
        """
        self.url = url
        self.year = year
        self.get_details = get_details
        self.headless = headless
        self.timeout = timeout
        self.results = []

        # è§£ææœŸå·
        if isinstance(issues, str):
            self.issues = self.parse_issue_string(issues)
        elif isinstance(issues, int):
            self.issues = [issues]
        elif isinstance(issues, list):
            self.issues = sorted(set(issues))
        else:
            raise TypeError(f"ä¸æ”¯æŒçš„æœŸå·ç±»å‹: {type(issues)}")

        # éªŒè¯æœŸå·
        for issue in self.issues:
            if not 1 <= issue <= 12:
                raise ValueError(f"æœŸå· {issue} è¶…å‡ºæœ‰æ•ˆèŒƒå›´ (1-12)")

        # éªŒè¯ URL
        parsed = urlparse(url)
        if not parsed.scheme or not parsed.netloc:
            raise ValueError(f"æ— æ•ˆçš„ URL: {url}")

    def run(self, issue: Optional[int] = None) -> list:
        """
        è¿è¡Œçˆ¬è™«ï¼ˆå•æœŸï¼‰

        Args:
            issue: æœŸå·ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ç¬¬ä¸€æœŸ

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        target_issue = issue if issue is not None else self.issues[0]
        return self._crawl_single_issue(target_issue)

    def run_all_issues(self) -> dict:
        """
        è¿è¡Œçˆ¬è™«ï¼ˆå¤šæœŸï¼‰

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæœŸå·ï¼Œå€¼ä¸ºè®ºæ–‡åˆ—è¡¨
        """
        all_results = {}

        if not self.issues:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æœŸå·")
            return all_results

        # å¦‚æœåªæœ‰ä¸€æœŸï¼Œç›´æ¥è¿è¡Œ
        if len(self.issues) == 1:
            papers = self.run(self.issues[0])
            all_results[self.issues[0]] = papers
            return all_results

        print(f"å°†çˆ¬å– {self.year} å¹´ç¬¬ {self.issues[0]} è‡³ {self.issues[-1]} æœŸï¼Œå…± {len(self.issues)} æœŸ")

        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()

            try:
                # 1. è®¿é—®æœŸåˆŠå¯¼èˆªé¡µ
                print(f"æ­£åœ¨è®¿é—®: {self.url}")
                page.goto(self.url, timeout=self.timeout, wait_until="networkidle")

                # 2. å±•å¼€å¹´ä»½åˆ—è¡¨
                self._expand_year(page)

                # 3. éå†æ¯æœŸ
                for i, issue in enumerate(self.issues):
                    print(f"\n{'='*50}")
                    print(f"æ­£åœ¨çˆ¬å– {self.year} å¹´ç¬¬ {issue} æœŸ ({i+1}/{len(self.issues)})")
                    print(f"{'='*50}")

                    try:
                        # é€‰æ‹©æœŸå·
                        self._select_issue(page, issue)

                        # ç­‰å¾…è®ºæ–‡åˆ—è¡¨åŠ è½½
                        self._wait_for_papers(page)

                        # çˆ¬å–è®ºæ–‡åˆ—è¡¨
                        papers = self._extract_papers(page, issue)

                        # å¦‚æœéœ€è¦è·å–è¯¦æƒ…
                        if self.get_details and papers:
                            papers = self._get_paper_details(page, papers)

                        all_results[issue] = papers
                        self.results.extend(papers)

                    except Exception as e:
                        print(f"çˆ¬å– {self.year} å¹´ç¬¬ {issue} æœŸæ—¶å‡ºé”™: {e}")
                        all_results[issue] = []

            except PlaywrightTimeoutError as e:
                print(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
                raise
            except Exception as e:
                print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                raise
            finally:
                browser.close()

        return all_results

    async def run_all_issues_async(self, concurrency: int = 3) -> dict:
        """
        å¼‚æ­¥è¿è¡Œçˆ¬è™«ï¼ˆå¤šæœŸï¼‰- é«˜æ€§èƒ½ç‰ˆæœ¬

        Args:
            concurrency: å¹¶å‘æ•°ï¼Œé»˜è®¤ 3

        Returns:
            å­—å…¸ï¼Œé”®ä¸ºæœŸå·ï¼Œå€¼ä¸ºè®ºæ–‡åˆ—è¡¨
        """
        all_results = {}

        if not self.issues:
            print("è­¦å‘Š: æ²¡æœ‰æœ‰æ•ˆçš„æœŸå·")
            return all_results

        # å¦‚æœåªæœ‰ä¸€æœŸï¼Œç›´æ¥è¿è¡Œ
        if len(self.issues) == 1:
            papers = await self._crawl_single_issue_async(self.issues[0], concurrency)
            all_results[self.issues[0]] = papers
            return all_results

        print(f"å°†çˆ¬å– {self.year} å¹´ç¬¬ {self.issues[0]} è‡³ {self.issues[-1]} æœŸï¼Œå…± {len(self.issues)} æœŸ")

        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            try:
                # 1. è®¿é—®æœŸåˆŠå¯¼èˆªé¡µ
                print(f"æ­£åœ¨è®¿é—®: {self.url}")
                await page.goto(self.url, timeout=self.timeout, wait_until="networkidle")

                # 2. å±•å¼€å¹´ä»½åˆ—è¡¨
                await self._expand_year_async(page)

                # 3. éå†æ¯æœŸ
                for i, issue in enumerate(self.issues):
                    print(f"\n{'='*50}")
                    print(f"æ­£åœ¨çˆ¬å– {self.year} å¹´ç¬¬ {issue} æœŸ ({i+1}/{len(self.issues)})")
                    print(f"{'='*50}")

                    try:
                        # é€‰æ‹©æœŸå·
                        await self._select_issue_async(page, issue)

                        # ç­‰å¾…è®ºæ–‡åˆ—è¡¨åŠ è½½
                        await self._wait_for_papers_async(page)

                        # çˆ¬å–è®ºæ–‡åˆ—è¡¨
                        papers = await self._extract_papers_async(page, issue)

                        # å¦‚æœéœ€è¦è·å–è¯¦æƒ…ï¼ˆå¼‚æ­¥å¹¶å‘ï¼‰
                        if self.get_details and papers:
                            papers = await self._get_paper_details_async(context, papers, concurrency)

                        all_results[issue] = papers
                        self.results.extend(papers)

                    except Exception as e:
                        print(f"çˆ¬å– {self.year} å¹´ç¬¬ {issue} æœŸæ—¶å‡ºé”™: {e}")
                        all_results[issue] = []

            except AsyncPlaywrightTimeoutError as e:
                print(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
                raise
            except Exception as e:
                print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                raise
            finally:
                await browser.close()

        return all_results

    async def _crawl_single_issue_async(self, issue: int, concurrency: int = 3) -> list:
        """
        å¼‚æ­¥çˆ¬å–å•æœŸè®ºæ–‡

        Args:
            issue: æœŸå·
            concurrency: å¹¶å‘æ•°

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        async with async_playwright() as p:
            browser = await p.chromium.launch(headless=self.headless)
            context = await browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = await context.new_page()

            try:
                # 1. è®¿é—®æœŸåˆŠå¯¼èˆªé¡µ
                print(f"æ­£åœ¨è®¿é—®: {self.url}")
                await page.goto(self.url, timeout=self.timeout, wait_until="networkidle")

                # 2. å±•å¼€å¹´ä»½åˆ—è¡¨
                await self._expand_year_async(page)

                # 3. é€‰æ‹©æœŸå·
                await self._select_issue_async(page, issue)

                # 4. ç­‰å¾…è®ºæ–‡åˆ—è¡¨åŠ è½½
                await self._wait_for_papers_async(page)

                # 5. çˆ¬å–è®ºæ–‡åˆ—è¡¨
                papers = await self._extract_papers_async(page, issue)

                # 6. å¦‚æœéœ€è¦è·å–è¯¦æƒ…
                if self.get_details and papers:
                    papers = await self._get_paper_details_async(context, papers, concurrency)

                self.results = papers
                return papers

            except AsyncPlaywrightTimeoutError as e:
                print(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
                raise
            except Exception as e:
                print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                raise
            finally:
                await browser.close()

    async def _expand_year_async(self, page):
        """å±•å¼€æŒ‡å®šå¹´ä»½çš„åˆ—è¡¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        year_str = str(self.year)

        try:
            # æ–¹æ³•1: ç›´æ¥ç‚¹å‡»å¹´ä»½å…ƒç´ 
            year_dt = page.locator(f"dt:has-text('{year_str}')")
            count = await year_dt.count()
            if count > 0:
                await year_dt.first.click()
                await asyncio.sleep(0.5)
                print(f"å·²ç‚¹å‡»å¹´ä»½: {year_str}")
                return

            # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«å¹´ä»½çš„ dt å…ƒç´ 
            all_dts = page.locator("dt")
            count = await all_dts.count()
            for i in range(count):
                dt = all_dts.nth(i)
                dt_text = await dt.inner_text()
                if year_str in dt_text:
                    await dt.click()
                    await asyncio.sleep(0.5)
                    print(f"å·²ç‚¹å‡»å¹´ä»½: {year_str}")
                    return

            # æ–¹æ³•3: æŸ¥æ‰¾å¹´ä»½å¯¹åº”çš„ dl å…ƒç´ å¹¶å±•å¼€
            year_dl = page.locator(f"dl[id*='{year_str}']")
            count = await year_dl.count()
            if count > 0:
                dt = year_dl.locator("dt")
                dt_count = await dt.count()
                if dt_count > 0:
                    await dt.click()
                    await asyncio.sleep(0.5)
                    print(f"å·²ç‚¹å‡»å¹´ä»½: {year_str}")
                    return

            print(f"è­¦å‘Š: æœªæ‰¾åˆ°å¹´ä»½ {year_str}ï¼Œå°†å°è¯•ä½¿ç”¨å½“å‰å±•å¼€çš„æœŸå·")

        except Exception as e:
            print(f"å±•å¼€å¹´ä»½åˆ—è¡¨æ—¶å‡ºé”™: {e}")

    async def _select_issue_async(self, page, issue: Optional[int] = None):
        """é€‰æ‹©æŒ‡å®šæœŸå·ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        target_issue = issue if issue is not None else self.issues[0]
        issue_id = f"yq{self.year}{target_issue:02d}"
        print(f"æ­£åœ¨é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ (ID: {issue_id})")

        try:
            # å°è¯•é€šè¿‡ ID é€‰æ‹©
            issue_link = page.locator(f"#{issue_id}")
            count = await issue_link.count()
            if count > 0:
                await issue_link.first.click()
                await asyncio.sleep(1)
                print(f"å·²é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ")
                return

            # å°è¯•é€šè¿‡æ–‡æœ¬é€‰æ‹© (No.XX æ ¼å¼)
            issue_no = f"No.{target_issue}"
            all_issues = page.locator("a[id^='yq']")
            count = await all_issues.count()
            for i in range(count):
                link = all_issues.nth(i)
                link_text = await link.inner_text()
                if issue_no in link_text:
                    await link.click()
                    await asyncio.sleep(1)
                    print(f"å·²é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ")
                    return

            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            all_issues = page.locator("a[id^='yq']")
            count = await all_issues.count()
            for i in range(count):
                link = all_issues.nth(i)
                link_id = await link.get_attribute("id") or ""
                if f"{self.year}" in link_id:
                    link_text = await link.inner_text()
                    if f"{target_issue:02d}" in link_id or f"No.{target_issue}" in link_text:
                        await link.click()
                        await asyncio.sleep(1)
                        print(f"å·²é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ")
                        return

            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœŸå· {self.year}å¹´ç¬¬{target_issue}æœŸï¼Œå°†ä½¿ç”¨å½“å‰æ˜¾ç¤ºçš„æœŸå·")

        except Exception as e:
            print(f"é€‰æ‹©æœŸå·æ—¶å‡ºé”™: {e}")

    async def _wait_for_papers_async(self, page, max_wait: int = 10):
        """ç­‰å¾…è®ºæ–‡åˆ—è¡¨åŠ è½½ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        print("æ­£åœ¨åŠ è½½è®ºæ–‡åˆ—è¡¨...")
        try:
            paper_rows = page.locator("dd.row")
            count = 0
            for _ in range(max_wait):
                count = await paper_rows.count()
                if count > 0:
                    print(f"å·²æ‰¾åˆ° {count} ç¯‡è®ºæ–‡")
                    return
                await asyncio.sleep(1)
            print(f"ç­‰å¾…è¶…æ—¶ï¼Œå½“å‰æ‰¾åˆ° {count} ç¯‡è®ºæ–‡")
        except Exception as e:
            print(f"ç­‰å¾…è®ºæ–‡åˆ—è¡¨æ—¶å‡ºé”™: {e}")

    async def _extract_papers_async(self, page, issue: int) -> list:
        """æå–è®ºæ–‡åˆ—è¡¨ï¼ˆå¼‚æ­¥ç‰ˆæœ¬ï¼‰"""
        papers = []
        paper_rows = page.locator("dd.row")
        count = await paper_rows.count()

        print(f"æ­£åœ¨æå–è®ºæ–‡ä¿¡æ¯ (å…± {count} ç¯‡)...")

        for i in range(count):
            try:
                row = paper_rows.nth(i)

                # è·å–æ ‡é¢˜é“¾æ¥
                title_link = row.locator("span.name a")
                title = ""
                abstract_url = ""

                link_count = await title_link.count()
                if link_count > 0:
                    title = await title_link.inner_text()
                    title = title.strip()
                    abstract_url = await title_link.get_attribute("href") or ""

                # è·å–ä½œè€…
                author_span = row.locator("span.author")
                author = ""
                author_count = await author_span.count()
                if author_count > 0:
                    author = await author_span.inner_text()
                    author = author.strip()

                # è·å–é¡µç 
                company_span = row.locator("span.company")
                pages = ""
                company_count = await company_span.count()
                if company_count > 0:
                    pages = await company_span.inner_text()
                    pages = pages.strip()

                paper = {
                    "year": self.year,
                    "issue": issue,
                    "title": title,
                    "author": author,
                    "pages": pages,
                    "abstract_url": abstract_url,
                    "abstract": "" if self.get_details else None
                }

                papers.append(paper)

            except Exception as e:
                print(f"æå–ç¬¬ {i+1} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                continue

        print(f"å·²æå– {len(papers)} ç¯‡è®ºæ–‡")
        return papers

    async def _get_paper_details_async(self, context, papers: list, concurrency: int = 3) -> list:
        """
        å¼‚æ­¥è·å–è®ºæ–‡æ‘˜è¦è¯¦æƒ…ï¼ˆå¹¶å‘ï¼‰

        Args:
            context: æµè§ˆå™¨ä¸Šä¸‹æ–‡
            papers: è®ºæ–‡åˆ—è¡¨
            concurrency: å¹¶å‘æ•°

        Returns:
            æ›´æ–°åçš„è®ºæ–‡åˆ—è¡¨
        """
        total = len(papers)
        print(f"\næ­£åœ¨è·å– {total} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯ (å¹¶å‘æ•°: {concurrency})...")

        # åˆ›å»ºè¿›åº¦æŠ¥å‘Šå™¨
        reporter = ProgressReporter(total=total, stage="detail")

        # è¿›åº¦å›è°ƒå‡½æ•°
        def progress_callback(r: ProgressReporter, paper: dict):
            r.report(paper.get("title", ""))

        # åˆ›å»ºä¿¡å·é‡æ§åˆ¶å¹¶å‘
        semaphore = asyncio.Semaphore(concurrency)

        # åˆ›å»ºå¼‚æ­¥è¯¦æƒ…çˆ¬è™«
        detail_spider = AsyncPaperDetailSpider(
            semaphore=semaphore,
            progress_callback=progress_callback,
            timeout=self.timeout
        )

        # è®¾ç½®æŠ¥å‘Šå™¨
        detail_spider.reporter = reporter

        # æ‰¹é‡è·å–è¯¦æƒ…
        papers = await detail_spider.fetch_details_batch(context, papers)

        # æœ€ç»ˆè¿›åº¦è¾“å‡º
        progress = reporter.get_progress()
        print(f"\næ‘˜è¦è·å–å®Œæˆ: æˆåŠŸ {progress['success']} ç¯‡ï¼Œå¤±è´¥ {progress['failed']} ç¯‡ï¼Œè·³è¿‡ {progress['skipped']} ç¯‡")

        return papers

    def _crawl_single_issue(self, issue: int) -> list:
        """
        çˆ¬å–å•æœŸè®ºæ–‡

        Args:
            issue: æœŸå·

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=self.headless)
            context = browser.new_context(
                user_agent="Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36"
            )
            page = context.new_page()

            try:
                # 1. è®¿é—®æœŸåˆŠå¯¼èˆªé¡µ
                print(f"æ­£åœ¨è®¿é—®: {self.url}")
                page.goto(self.url, timeout=self.timeout, wait_until="networkidle")

                # 2. å±•å¼€å¹´ä»½åˆ—è¡¨
                self._expand_year(page)

                # 3. é€‰æ‹©æœŸå·
                self._select_issue(page, issue)

                # 4. ç­‰å¾…è®ºæ–‡åˆ—è¡¨åŠ è½½
                self._wait_for_papers(page)

                # 5. çˆ¬å–è®ºæ–‡åˆ—è¡¨
                papers = self._extract_papers(page, issue)

                # 6. å¦‚æœéœ€è¦è·å–è¯¦æƒ…
                if self.get_details and papers:
                    papers = self._get_paper_details(page, papers)

                self.results = papers
                return papers

            except PlaywrightTimeoutError as e:
                print(f"é¡µé¢åŠ è½½è¶…æ—¶: {e}")
                raise
            except Exception as e:
                print(f"çˆ¬å–è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {e}")
                raise
            finally:
                browser.close()

    def _expand_year(self, page):
        """å±•å¼€æŒ‡å®šå¹´ä»½çš„åˆ—è¡¨"""
        year_str = str(self.year)
        selector = f"#Year_Issue"

        # å°è¯•å¤šç§æ–¹å¼å±•å¼€å¹´ä»½
        try:
            # æ–¹æ³•1: ç›´æ¥ç‚¹å‡»å¹´ä»½å…ƒç´ 
            year_dt = page.locator(f"dt:has-text('{year_str}')")
            if year_dt.count() > 0:
                year_dt.first.click()
                time.sleep(0.5)
                print(f"å·²ç‚¹å‡»å¹´ä»½: {year_str}")
                return

            # æ–¹æ³•2: æŸ¥æ‰¾åŒ…å«å¹´ä»½çš„ dt å…ƒç´ 
            all_dts = page.locator("dt")
            for i in range(all_dts.count()):
                dt = all_dts.nth(i)
                dt_text = dt.inner_text()
                if year_str in dt_text:
                    dt.click()
                    time.sleep(0.5)
                    print(f"å·²ç‚¹å‡»å¹´ä»½: {year_str}")
                    return

            # æ–¹æ³•3: æŸ¥æ‰¾å¹´ä»½å¯¹åº”çš„ dl å…ƒç´ å¹¶å±•å¼€
            year_dl = page.locator(f"dl[id*='{year_str}']")
            if year_dl.count() > 0:
                dt = year_dl.locator("dt")
                if dt.count() > 0:
                    dt.click()
                    time.sleep(0.5)
                    print(f"å·²ç‚¹å‡»å¹´ä»½: {year_str}")
                    return

            print(f"è­¦å‘Š: æœªæ‰¾åˆ°å¹´ä»½ {year_str}ï¼Œå°†å°è¯•ä½¿ç”¨å½“å‰å±•å¼€çš„æœŸå·")

        except Exception as e:
            print(f"å±•å¼€å¹´ä»½åˆ—è¡¨æ—¶å‡ºé”™: {e}")

    def _select_issue(self, page, issue: Optional[int] = None):
        """
        é€‰æ‹©æŒ‡å®šæœŸå·

        Args:
            page: Playwright é¡µé¢å¯¹è±¡
            issue: æœŸå·ï¼Œå¦‚æœä¸º None åˆ™ä½¿ç”¨ self.issues[0]
        """
        target_issue = issue if issue is not None else self.issues[0]
        issue_id = f"yq{self.year}{target_issue:02d}"
        print(f"æ­£åœ¨é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ (ID: {issue_id})")

        try:
            # å°è¯•é€šè¿‡ ID é€‰æ‹©
            issue_link = page.locator(f"#{issue_id}")
            if issue_link.count() > 0:
                issue_link.first.click()
                time.sleep(1)
                print(f"å·²é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ")
                return

            # å°è¯•é€šè¿‡æ–‡æœ¬é€‰æ‹© (No.XX æ ¼å¼)
            issue_no = f"No.{target_issue}"
            all_issues = page.locator("a[id^='yq']")
            for i in range(all_issues.count()):
                link = all_issues.nth(i)
                link_text = link.inner_text()
                if issue_no in link_text:
                    link.click()
                    time.sleep(1)
                    print(f"å·²é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ")
                    return

            # å°è¯•æ¨¡ç³ŠåŒ¹é…
            all_issues = page.locator("a[id^='yq']")
            for i in range(all_issues.count()):
                link = all_issues.nth(i)
                link_id = link.get_attribute("id") or ""
                if f"{self.year}" in link_id:
                    link_text = link.inner_text()
                    if f"{target_issue:02d}" in link_id or f"No.{target_issue}" in link_text:
                        link.click()
                        time.sleep(1)
                        print(f"å·²é€‰æ‹©æœŸå·: {self.year}å¹´ç¬¬{target_issue}æœŸ")
                        return

            print(f"è­¦å‘Š: æœªæ‰¾åˆ°æœŸå· {self.year}å¹´ç¬¬{target_issue}æœŸï¼Œå°†ä½¿ç”¨å½“å‰æ˜¾ç¤ºçš„æœŸå·")

        except Exception as e:
            print(f"é€‰æ‹©æœŸå·æ—¶å‡ºé”™: {e}")

    def _wait_for_papers(self, page, max_wait: int = 10):
        """ç­‰å¾…è®ºæ–‡åˆ—è¡¨åŠ è½½"""
        print("æ­£åœ¨åŠ è½½è®ºæ–‡åˆ—è¡¨...")
        try:
            # ç­‰å¾…è®ºæ–‡è¡Œå…ƒç´ å‡ºç°
            paper_rows = page.locator("dd.row")
            count = 0
            for _ in range(max_wait):
                count = paper_rows.count()
                if count > 0:
                    print(f"å·²æ‰¾åˆ° {count} ç¯‡è®ºæ–‡")
                    return
                time.sleep(1)
            print(f"ç­‰å¾…è¶…æ—¶ï¼Œå½“å‰æ‰¾åˆ° {count} ç¯‡è®ºæ–‡")
        except Exception as e:
            print(f"ç­‰å¾…è®ºæ–‡åˆ—è¡¨æ—¶å‡ºé”™: {e}")

    def _extract_papers(self, page, issue: int) -> list:
        """
        æå–è®ºæ–‡åˆ—è¡¨

        Args:
            page: Playwright é¡µé¢å¯¹è±¡
            issue: æœŸå·

        Returns:
            è®ºæ–‡åˆ—è¡¨
        """
        papers = []
        paper_rows = page.locator("dd.row")

        print(f"æ­£åœ¨æå–è®ºæ–‡ä¿¡æ¯ (å…± {paper_rows.count()} ç¯‡)...")

        for i in range(paper_rows.count()):
            try:
                row = paper_rows.nth(i)

                # è·å–æ ‡é¢˜é“¾æ¥
                title_link = row.locator("span.name a")
                title = ""
                abstract_url = ""

                if title_link.count() > 0:
                    title = title_link.inner_text().strip()
                    abstract_url = title_link.get_attribute("href") or ""

                # è·å–ä½œè€…
                author_span = row.locator("span.author")
                author = ""
                if author_span.count() > 0:
                    author = author_span.inner_text().strip()

                # è·å–é¡µç 
                company_span = row.locator("span.company")
                pages = ""
                if company_span.count() > 0:
                    pages = company_span.inner_text().strip()

                paper = {
                    "year": self.year,
                    "issue": issue,
                    "title": title,
                    "author": author,
                    "pages": pages,
                    "abstract_url": abstract_url,
                    "abstract": "" if self.get_details else None
                }

                papers.append(paper)

            except Exception as e:
                print(f"æå–ç¬¬ {i+1} ç¯‡è®ºæ–‡æ—¶å‡ºé”™: {e}")
                continue

        print(f"å·²æå– {len(papers)} ç¯‡è®ºæ–‡")
        return papers

    def _get_paper_details(self, page, papers: list) -> list:
        """è·å–è®ºæ–‡æ‘˜è¦è¯¦æƒ…"""
        total = len(papers)
        print(f"\næ­£åœ¨è·å– {total} ç¯‡è®ºæ–‡çš„è¯¦ç»†ä¿¡æ¯...")

        # ä½¿ç”¨ç‹¬ç«‹çš„è¯¦æƒ…çˆ¬å–æ¨¡å—
        detail_spider = PaperDetailSpider(timeout=self.timeout, delay=0.3)

        success_count = 0
        fail_count = 0
        skip_count = 0

        for i, paper in enumerate(papers):
            if not paper.get("abstract_url"):
                print(f"  [{i+1}/{total}] â­ï¸  è·³è¿‡: æ— æ‘˜è¦é“¾æ¥", flush=True)
                skip_count += 1
                continue

            try:
                title_short = paper['title'][:40] + "..." if len(paper['title']) > 40 else paper['title']
                print(f"  [{i+1}/{total}] ğŸ“„ {title_short}", end=" ", flush=True)

                # åœ¨æ–°æ ‡ç­¾é¡µæ‰“å¼€æ‘˜è¦é¡µ
                context = page.context
                detail_page = context.new_page()
                detail_page.set_default_timeout(self.timeout)

                # ä½¿ç”¨ç‹¬ç«‹æ¨¡å—è·å–è¯¦æƒ…
                detail = detail_spider.fetch_detail(detail_page, paper["abstract_url"])

                if detail:
                    paper["abstract"] = detail.get("abstract", "")
                    paper["keywords"] = detail.get("keywords", "")
                    paper["doi"] = detail.get("doi", "")
                    paper["fund"] = detail.get("fund", "")
                    paper["authors_detail"] = detail.get("authors", "")
                    print("âœ…", flush=True)
                    success_count += 1
                else:
                    paper["abstract"] = "è·å–å¤±è´¥"
                    print("âŒ", flush=True)
                    fail_count += 1

                detail_page.close()

            except PlaywrightTimeoutError:
                print("â±ï¸  è¶…æ—¶", flush=True)
                paper["abstract"] = "è·å–å¤±è´¥: è¶…æ—¶"
                fail_count += 1
            except Exception as e:
                print(f"âŒ é”™è¯¯: {e}", flush=True)
                paper["abstract"] = f"è·å–å¤±è´¥: {str(e)}"
                fail_count += 1

        print(f"\næ‘˜è¦è·å–å®Œæˆ: æˆåŠŸ {success_count} ç¯‡ï¼Œå¤±è´¥ {fail_count} ç¯‡ï¼Œè·³è¿‡ {skip_count} ç¯‡")
        return papers

    def save_results(self, filepath: str = "results.json"):
        """ä¿å­˜ç»“æœåˆ°æ–‡ä»¶"""
        output_path = Path(filepath)
        # è‡ªåŠ¨åˆ›å»ºç›®å½•
        output_path.parent.mkdir(parents=True, exist_ok=True)
        # ä½¿ç”¨ JSONSanitizer æ¸…ç†æ•°æ®åå†ä¿å­˜
        JSONSanitizer.sanitize_and_save(self.results, str(output_path))
        print(f"ç»“æœå·²ä¿å­˜åˆ°: {output_path.absolute()}")

    def print_results(self):
        """æ‰“å°ç»“æœåˆ°æ§åˆ¶å°"""
        for i, paper in enumerate(self.results, 1):
            print(f"\n[{i}] {paper['title']}")
            print(f"    å¹´ä»½: {paper.get('year', 'N/A')}")
            print(f"    æœŸå·: {paper.get('issue', 'N/A')}")
            print(f"    ä½œè€…: {paper['author']}")
            print(f"    é¡µç : {paper['pages']}")
            if paper.get('abstract'):
                abstract = paper['abstract']
                if len(abstract) > 200:
                    abstract = abstract[:200] + "..."
                print(f"    æ‘˜è¦: {abstract}")
            if paper.get('keywords'):
                print(f"    å…³é”®è¯: {paper['keywords']}")


def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(
        description="CNKI æœŸåˆŠè®ºæ–‡çˆ¬è™« - ä½¿ç”¨ Playwright",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # çˆ¬å– 2025 å¹´ç¬¬ 6 æœŸè®ºæ–‡åˆ—è¡¨
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i 6

  # çˆ¬å– 2025 å¹´ç¬¬ 1-3 æœŸè®ºæ–‡åˆ—è¡¨ï¼ˆèŒƒå›´æ ¼å¼ï¼‰
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i "1-3"

  # çˆ¬å– 2025 å¹´ç¬¬ 1,5,7 æœŸè®ºæ–‡åˆ—è¡¨ï¼ˆç¦»æ•£æ ¼å¼ï¼‰
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i "1,5,7"

  # çˆ¬å– 2025 å¹´ç¬¬ 1-3,5,7-9 æœŸè®ºæ–‡åˆ—è¡¨ï¼ˆæ··åˆæ ¼å¼ï¼‰
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i "1-3,5,7-9"

  # çˆ¬å–å¹¶è·å–è®ºæ–‡æ‘˜è¦ (é»˜è®¤å·²å¼€å¯)
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i 6

  # ä¸è·å–è®ºæ–‡æ‘˜è¦
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i 6 --no-details

  # éæ— å¤´æ¨¡å¼è¿è¡Œï¼ˆæ˜¾ç¤ºæµè§ˆå™¨ï¼‰
  python cnki_spider.py -u "https://navi.cnki.net/knavi/journals/ZGTS/detail" -y 2025 -i 6 --no-headless
        """
    )

    parser.add_argument(
        "-u", "--url",
        required=True,
        help="æœŸåˆŠå¯¼èˆªé¡µ URL (å¦‚: https://navi.cnki.net/knavi/journals/ZGTS/detail)"
    )

    parser.add_argument(
        "-y", "--year",
        type=int,
        required=True,
        help="è¦çˆ¬å–çš„å¹´ä»½"
    )

    parser.add_argument(
        "-i", "--issue",
        type=str,
        required=True,
        help="è¦çˆ¬å–çš„æœŸå·ï¼Œæ”¯æŒä»¥ä¸‹æ ¼å¼:\n"
             "  - å•æœŸ: 6\n"
             "  - èŒƒå›´: 1-3 (è¡¨ç¤º 1,2,3 æœŸ)\n"
             "  - ç¦»æ•£: 1,5,7 (è¡¨ç¤º 1,5,7 æœŸ)\n"
             "  - æ··åˆ: 1-3,5,7-9"
    )

    parser.add_argument(
        "-d", "--details",
        action="store_true",
        default=True,
        help="æ˜¯å¦è·å–è®ºæ–‡æ‘˜è¦ç­‰è¯¦ç»†ä¿¡æ¯ (é»˜è®¤: è·å–)"
    )

    parser.add_argument(
        "--no-details",
        dest="details",
        action="store_false",
        help="ä¸è·å–è®ºæ–‡æ‘˜è¦ç­‰è¯¦ç»†ä¿¡æ¯"
    )

    parser.add_argument(
        "--no-headless",
        action="store_true",
        help="éæ— å¤´æ¨¡å¼è¿è¡Œï¼Œæ˜¾ç¤ºæµè§ˆå™¨çª—å£"
    )

    parser.add_argument(
        "-t", "--timeout",
        type=int,
        default=30000,
        help="é¡µé¢åŠ è½½è¶…æ—¶æ—¶é—´ï¼ˆæ¯«ç§’ï¼‰ï¼Œé»˜è®¤ 30000"
    )

    parser.add_argument(
        "-o", "--output",
        type=str,
        default="results.json",
        help="è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼Œé»˜è®¤ results.json"
    )

    parser.add_argument(
        "-c", "--concurrency",
        type=int,
        default=3,
        help="å¼‚æ­¥å¹¶å‘æ•°ï¼Œé»˜è®¤ 3ï¼ˆä»…å¼‚æ­¥æ¨¡å¼æœ‰æ•ˆï¼‰"
    )

    parser.add_argument(
        "--sync",
        action="store_true",
        help="ä½¿ç”¨åŒæ­¥æ¨¡å¼ï¼ˆé»˜è®¤ä¸ºå¼‚æ­¥æ¨¡å¼ï¼‰"
    )

    args = parser.parse_args()

    # è§£ææœŸå·å­—ç¬¦ä¸²
    try:
        issues = CNKISpider.parse_issue_string(args.issue)
        print(f"è§£ææœŸå·: {args.issue} -> {issues}")
    except ValueError as e:
        print(f"é”™è¯¯: {e}")
        sys.exit(1)

    # åˆ›å»ºçˆ¬è™«
    spider = CNKISpider(
        url=args.url,
        year=args.year,
        issues=issues,
        get_details=args.details,
        headless=not args.no_headless,
        timeout=args.timeout
    )

    try:
        # æ ¹æ®å‚æ•°é€‰æ‹©åŒæ­¥æˆ–å¼‚æ­¥æ¨¡å¼
        if args.sync:
            # åŒæ­¥æ¨¡å¼
            print("ä½¿ç”¨åŒæ­¥æ¨¡å¼...")
            if len(issues) == 1:
                papers = spider.run()
            else:
                all_results = spider.run_all_issues()
                papers = spider.results
        else:
            # å¼‚æ­¥æ¨¡å¼ï¼ˆé»˜è®¤ï¼‰
            print(f"ä½¿ç”¨å¼‚æ­¥æ¨¡å¼ï¼ˆå¹¶å‘æ•°: {args.concurrency}ï¼‰...")
            if len(issues) == 1:
                papers = asyncio.run(spider._crawl_single_issue_async(issues[0], args.concurrency))
            else:
                all_results = asyncio.run(spider.run_all_issues_async(args.concurrency))
                papers = spider.results

        if papers:
            # æ‰“å°ç»“æœ
            spider.print_results()

            # ä¿å­˜ç»“æœ
            spider.save_results(args.output)

            print(f"\nâœ… æˆåŠŸçˆ¬å– {len(papers)} ç¯‡è®ºæ–‡")
        else:
            print("\nâš ï¸ æœªæ‰¾åˆ°ä»»ä½•è®ºæ–‡")

    except KeyboardInterrupt:
        print("\n\nâ¹ï¸ ç”¨æˆ·ä¸­æ–­æ‰§è¡Œ")
        sys.exit(0)
    except Exception as e:
        print(f"\nâŒ é”™è¯¯: {e}")
        sys.exit(1)


if __name__ == "__main__":
    main()
